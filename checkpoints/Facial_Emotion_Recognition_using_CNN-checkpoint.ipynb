{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztky6L9E-lnP"
   },
   "source": [
    "# **Facial Emotion Recognition**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dataset - https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHrzqEejOjR9"
   },
   "outputs": [],
   "source": [
    "_author_ = \" Prudhvi_GNV \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CULEspsFALh"
   },
   "source": [
    "**kaggle --account--Api**\n",
    "API's are used to access data from kaggle\n",
    "1. kaggle a/c -> my profile -> create api token\n",
    "2. In colabs ..while uploading api file... go to /content\n",
    "3. make .kaggle directory in /root \n",
    "4. copy kaggle api from /content to /root/.kaggle\n",
    "5. kaggle cmd(find in kaggle dataset page-> data) to download\n",
    "6. unzip the downloaded dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tbi7hj_R8_yD",
    "outputId": "821b2a4d-cf9c-4511-db4e-81d97a403842"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ASUS\\\\OneDrive - American International University-Bangladesh\\\\Desktop\\\\Python\\\\CVPR\\\\Facial-Emotion-Recognition-using-CNN-master'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nt4jf1OW_6gf",
    "outputId": "19a22d24-44f2-4198-a522-706ec250a91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '/root'\n",
      "C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n"
     ]
    }
   ],
   "source": [
    "cd /root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEY-3HG5AHFB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "mkdir .kaggle               # .kaggle not /.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cNO-HFFq85a0",
    "outputId": "d4247e10-6aab-422b-a1bc-2dc90ed1a465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/root/.kaggle'\n",
      "C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n"
     ]
    }
   ],
   "source": [
    "cd /root/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqSXEsJ29HP7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cp /content/kaggle.json ~/.kaggle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bN48VBLQpQb3",
    "outputId": "ca46b866-1ee6-4b15-d5e4-b8c04257c39f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ASUS\\\\OneDrive - American International University-Bangladesh\\\\Desktop\\\\Python\\\\CVPR\\\\Facial-Emotion-Recognition-using-CNN-master'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3YSkULw_pT0c",
    "outputId": "80e39c48-8bb8-49ab-c1cd-857aa1cca10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BE15-D154\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n",
      "\n",
      "04/19/2022  12:51 AM    <DIR>          .\n",
      "04/19/2022  12:51 AM    <DIR>          ..\n",
      "04/19/2022  12:51 AM    <DIR>          .ipynb_checkpoints\n",
      "09/11/2020  06:49 PM         5,126,251 Facial_Emotion_Recognition_using_CNN.ipynb\n",
      "09/11/2020  06:49 PM           837,462 haarcascade_frontalface_alt2.xml\n",
      "09/11/2020  06:49 PM           963,439 haarcascade_frontalface_default.xml\n",
      "09/11/2020  06:49 PM    <DIR>          images\n",
      "09/11/2020  06:49 PM             1,068 LICENSE\n",
      "09/11/2020  06:49 PM         2,651,440 model.h5\n",
      "09/11/2020  06:49 PM            13,518 model.json\n",
      "09/11/2020  06:49 PM             5,439 README.md\n",
      "               7 File(s)      9,598,617 bytes\n",
      "               4 Dir(s)  28,021,080,064 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Vlkz9IP9otV",
    "outputId": "f83ebad3-f048-47f0-d3ee-9b3b8f18a37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BE15-D154\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n",
      "\n",
      "04/19/2022  12:51 AM    <DIR>          .\n",
      "04/19/2022  12:51 AM    <DIR>          ..\n",
      "04/19/2022  12:51 AM    <DIR>          .ipynb_checkpoints\n",
      "09/11/2020  06:49 PM         5,126,251 Facial_Emotion_Recognition_using_CNN.ipynb\n",
      "09/11/2020  06:49 PM           837,462 haarcascade_frontalface_alt2.xml\n",
      "09/11/2020  06:49 PM           963,439 haarcascade_frontalface_default.xml\n",
      "09/11/2020  06:49 PM    <DIR>          images\n",
      "09/11/2020  06:49 PM             1,068 LICENSE\n",
      "09/11/2020  06:49 PM         2,651,440 model.h5\n",
      "09/11/2020  06:49 PM            13,518 model.json\n",
      "09/11/2020  06:49 PM             5,439 README.md\n",
      "               7 File(s)      9,598,617 bytes\n",
      "               4 Dir(s)  28,021,014,528 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "LEkxCltOmE5v",
    "outputId": "30fc2762-aa92-452e-f9fd-4c5525d13f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=ea10d63aee4cd7e9fc64471a78e2b4eb6d8253bb6b8d7d3a6e421bc0f15391b7\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\29\\da\\11\\144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.12\n"
     ]
    }
   ],
   "source": [
    "# if ..looks like outdated api ..server version ERROR occurs\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytfZi3nJmilT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#if ..only read permisions ..ERROR\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "yTPl804d77nk",
    "outputId": "e3bf67fe-e747-4999-8321-ecfc11f9840d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\asus\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\asus\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Scripts\\kaggle.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\kaggle\\__init__.py\", line 19, in <module>\n",
      "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
      "  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\kaggle\\api\\__init__.py\", line 22, in <module>\n",
      "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
      "  File \"c:\\users\\asus\\anaconda3\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py\", line 75, in <module>\n",
      "    from slugify import slugify\n",
      "ModuleNotFoundError: No module named 'slugify'\n"
     ]
    }
   ],
   "source": [
    "# if 403 forbidden ERROR ... then accept the rules in kaggle dataset page\n",
    "\n",
    "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "grCSTKE-_UG3",
    "outputId": "c23805f9-096b-42b4-aa2f-ecf05809922e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BE15-D154\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n",
      "\n",
      "04/19/2022  12:51 AM    <DIR>          .\n",
      "04/19/2022  12:51 AM    <DIR>          ..\n",
      "04/19/2022  12:51 AM    <DIR>          .ipynb_checkpoints\n",
      "09/11/2020  06:49 PM         5,126,251 Facial_Emotion_Recognition_using_CNN.ipynb\n",
      "09/11/2020  06:49 PM           837,462 haarcascade_frontalface_alt2.xml\n",
      "09/11/2020  06:49 PM           963,439 haarcascade_frontalface_default.xml\n",
      "09/11/2020  06:49 PM    <DIR>          images\n",
      "09/11/2020  06:49 PM             1,068 LICENSE\n",
      "09/11/2020  06:49 PM         2,651,440 model.h5\n",
      "09/11/2020  06:49 PM            13,518 model.json\n",
      "09/11/2020  06:49 PM             5,439 README.md\n",
      "               7 File(s)      9,598,617 bytes\n",
      "               4 Dir(s)  28,019,118,080 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "MEobtx6O9qvT",
    "outputId": "d2252ea3-ed19-49c4-86f5-4a248f8e9082"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "fAiYFWW7_vMT",
    "outputId": "ce44cebc-d1e3-4f07-9b65-c38ffc0d2b4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BE15-D154\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n",
      "\n",
      "04/19/2022  12:51 AM    <DIR>          .\n",
      "04/19/2022  12:51 AM    <DIR>          ..\n",
      "04/19/2022  12:51 AM    <DIR>          .ipynb_checkpoints\n",
      "09/11/2020  06:49 PM         5,126,251 Facial_Emotion_Recognition_using_CNN.ipynb\n",
      "09/11/2020  06:49 PM           837,462 haarcascade_frontalface_alt2.xml\n",
      "09/11/2020  06:49 PM           963,439 haarcascade_frontalface_default.xml\n",
      "09/11/2020  06:49 PM    <DIR>          images\n",
      "09/11/2020  06:49 PM             1,068 LICENSE\n",
      "09/11/2020  06:49 PM         2,651,440 model.h5\n",
      "09/11/2020  06:49 PM            13,518 model.json\n",
      "09/11/2020  06:49 PM             5,439 README.md\n",
      "               7 File(s)      9,598,617 bytes\n",
      "               4 Dir(s)  28,019,118,080 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQgMssSBwypP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'fer2013.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "!tar -xf fer2013.tar.gz   # x= extract , f= to create archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "tiO3ojiRw4wW",
    "outputId": "e0e40a5c-a7b9-4b91-9082-966b73da2aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BE15-D154\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n",
      "\n",
      "04/19/2022  12:51 AM    <DIR>          .\n",
      "04/19/2022  12:51 AM    <DIR>          ..\n",
      "04/19/2022  12:51 AM    <DIR>          .ipynb_checkpoints\n",
      "09/11/2020  06:49 PM         5,126,251 Facial_Emotion_Recognition_using_CNN.ipynb\n",
      "09/11/2020  06:49 PM           837,462 haarcascade_frontalface_alt2.xml\n",
      "09/11/2020  06:49 PM           963,439 haarcascade_frontalface_default.xml\n",
      "09/11/2020  06:49 PM    <DIR>          images\n",
      "09/11/2020  06:49 PM             1,068 LICENSE\n",
      "09/11/2020  06:49 PM         2,651,440 model.h5\n",
      "09/11/2020  06:49 PM            13,518 model.json\n",
      "09/11/2020  06:49 PM             5,439 README.md\n",
      "               7 File(s)      9,598,617 bytes\n",
      "               4 Dir(s)  28,019,089,408 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LIq55w1FF5fQ",
    "outputId": "04b0e543-2e6f-4f3c-f969-eba890e9be24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'fer2013'\n",
      "C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n"
     ]
    }
   ],
   "source": [
    "cd fer2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YaVtewzUF-zA",
    "outputId": "a534b5ca-77cd-488c-cfd2-2c6e1af274cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BE15-D154\n",
      "\n",
      " Directory of C:\\Users\\ASUS\\OneDrive - American International University-Bangladesh\\Desktop\\Python\\CVPR\\Facial-Emotion-Recognition-using-CNN-master\n",
      "\n",
      "04/19/2022  12:51 AM    <DIR>          .\n",
      "04/19/2022  12:51 AM    <DIR>          ..\n",
      "04/19/2022  12:51 AM    <DIR>          .ipynb_checkpoints\n",
      "09/11/2020  06:49 PM         5,126,251 Facial_Emotion_Recognition_using_CNN.ipynb\n",
      "09/11/2020  06:49 PM           837,462 haarcascade_frontalface_alt2.xml\n",
      "09/11/2020  06:49 PM           963,439 haarcascade_frontalface_default.xml\n",
      "09/11/2020  06:49 PM    <DIR>          images\n",
      "09/11/2020  06:49 PM             1,068 LICENSE\n",
      "09/11/2020  06:49 PM         2,651,440 model.h5\n",
      "09/11/2020  06:49 PM            13,518 model.json\n",
      "09/11/2020  06:49 PM             5,439 README.md\n",
      "               7 File(s)      9,598,617 bytes\n",
      "               4 Dir(s)  28,018,982,912 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YeU4HHZ_bS4"
   },
   "source": [
    "## **Dataset**\n",
    "* The data consists of 48x48 pixel grayscale images of faces. \n",
    " \n",
    "* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "* Dataset contains two columns, \"emotion\" and \"pixels\".\n",
    "* The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. \n",
    "* The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Zu89RTzBMty"
   },
   "source": [
    "# Data Visualization\n",
    "\n",
    "* powerful visualization tools => matplotlib, seaborn\n",
    "      1. matplotlib works on pandas dataframe\n",
    "      2.  matplotlib => basic ploting =>bars,pie,scatter, lines etc.. => used for MATLAB like graphs\n",
    "      3. seaborn => statistical ploting => depends on matplotlib...high level => provides default templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "xcfiDv03-cGX",
    "outputId": "b97002a4-fa58-4cf7-d3d0-8e4b7394ef20"
   },
   "outputs": [],
   "source": [
    "\"\"\" IMPORT ALL DEPENDENCIES\"\"\"\n",
    "\n",
    "\n",
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "# import color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "iTJESy7QA-5E",
    "outputId": "624d7579-649e-4869-f3e8-b9ef0ffca898"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/.kaggle/fer2013/fer2013.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-af19af0acc45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/root/.kaggle/fer2013/fer2013.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.kaggle/fer2013/fer2013.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/root/.kaggle/fer2013/fer2013.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "gbnhU1whADCT",
    "outputId": "4c45e608-8b5b-4a63-a0ab-c75f37ec8b69"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "prR_SJxiBA8U",
    "outputId": "df4086bb-a7fb-4fcb-fd1a-755a89841b01"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k39FElpEMVju"
   },
   "source": [
    "## **Plotting emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "ZidMumXvBzpr",
    "outputId": "5dca35ad-5622-495b-ed70-04b02154e64d"
   },
   "outputs": [],
   "source": [
    "\"\"\" matplotlib => to define size , sns => to use counterplot \"\"\"\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.countplot(x='emotion', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "7qEMmC3ILEtp",
    "outputId": "927affad-d1b7-4dcc-f584-7ebc7886d6cf"
   },
   "outputs": [],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wah5ra4rMB0O"
   },
   "source": [
    "* where ** 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iblqx4zrSuUY"
   },
   "source": [
    "### Observation\n",
    "* for Digust we have about 547 images only very less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4G1tEVgMnl0"
   },
   "source": [
    "## **Plotting Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "JL5-9TQJMAbi",
    "outputId": "4ab89dbf-dc7d-4b0f-8961-d5c3d50848ea"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "sns.countplot(x='Usage', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bTzE2z-2Lpsh",
    "outputId": "ff1d16a4-c894-4bbf-e5b7-ea366f344598"
   },
   "outputs": [],
   "source": [
    "df['Usage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_Xc1kb96J_I"
   },
   "source": [
    " **Intuition**\n",
    "\n",
    "* % matplotlib inline ==> magic cmd to setup IPython to display and store figures in notebooks\n",
    "* dataframe ==>pd.read_csv() , .head(), .tail(), .shape\n",
    "* count ==>  sns.countplot(x=,df) -> for graphs  ; df[col].value_counts() -> to display count values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLpNo_Wi78C3"
   },
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OshjJPMx95JF"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_size=(48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRs4m6UvM7AO"
   },
   "outputs": [],
   "source": [
    "pixels = df['pixels'].tolist() # Converting the relevant column element into a list for each row\n",
    "width, height = 48, 48\n",
    "faces = []\n",
    "\n",
    "for pixel_sequence in pixels:\n",
    "  face = [int(pixel) for pixel in pixel_sequence.split(' ')] # Splitting the string by space character as a list\n",
    "  face = np.asarray(face).reshape(width, height) #converting the list to numpy array in size of 48*48\n",
    "  face = cv2.resize(face.astype('uint8'),image_size) #resize the image to have 48 cols (width) and 48 rows (height)\n",
    "  faces.append(face.astype('float32')) #makes the list of each images of 48*48 and their pixels in numpyarray form\n",
    "  \n",
    "faces = np.asarray(faces) #converting the list into numpy array\n",
    "faces = np.expand_dims(faces, -1) #Expand the shape of an array -1=last dimension => means color space\n",
    "emotions = pd.get_dummies(df['emotion']).to_numpy() #doing the one hot encoding type on emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "VPsO_9sn77EB",
    "outputId": "e8f48f97-29d1-47dd-ed72-f41a24d1efb4"
   },
   "outputs": [],
   "source": [
    "print(faces[0]) #Pixels after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "s2JENcOV_nXJ",
    "outputId": "9d946358-ded7-4489-c937-28b8208204de"
   },
   "outputs": [],
   "source": [
    "print(faces.shape)\n",
    "print(faces[0].ndim)\n",
    "print(type(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yeQMsBoR-Y4d",
    "outputId": "d68aeb8b-f592-48aa-dd0d-8018b88a1572"
   },
   "outputs": [],
   "source": [
    "print(emotions[0]) #Emotion after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WOO-ZRSf-hgt",
    "outputId": "25f89382-4486-4098-a635-f66ae1dfd196"
   },
   "outputs": [],
   "source": [
    "print(emotions.shape)\n",
    "print(emotions.ndim)\n",
    "print(type(emotions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kj9OqVepeZ2E"
   },
   "source": [
    "**Intuition**\n",
    "* Pandas series(or col of dataframe) to list => tolist() =>gives list of string of values->each string is a image\n",
    "* split each string(image) in list and cvt them into list of list of integers\n",
    "* convert list into numpy array =>np.asarray(list)\n",
    "* numpy array has reshape(width,height) \n",
    "* cv2 has resize(nparray, (size)) -->cvt image into uint8(unsigned 8 bit integer) -> has range [0,255]\n",
    "  1. uint16 => [0,65535]\n",
    "  2. uint32 => 32 bit or 4 bytes => [0, 2^32]\n",
    "  3. float => [-1,1] or [0,1]\n",
    "  4. int8 => [-128, 127]\n",
    "  5. int =>  [-2^31 , 2^31 -1] \n",
    "* images => ntg but numpy array\n",
    "* openCV has 3 dimensions => width,height, color => color space = BGR ; dtype => by default uint8\n",
    "* reshape returns a copy [ doesn't change values] whereas resize changes the values[not return a copy]\n",
    "* np.expand_dims => inserts new axis => ntg but boxes(just think for convenience) => to increase dimensions => to expand shape\n",
    "  1. axis= 0 -> box to all values ; axis =1 -> boxes to each individual values\n",
    "  2. example , (2,1,2) =>[ [[[],[]]],[[[],[]]] ] => list has 2 boxes - each box has 1 box - inside that each 1 box has 2 boxes --> in general box is a list or value\n",
    "* pd.get_dummies => convert pd series into dummy coded dataframes or indicator variables. => categorical var into indicators vars. => s.no(row names) * values(cols names) - 0,1 values.\n",
    "* numpy.asmatrix(data,dtype) => interpret input as a matrix => doesn't return a copy of matrix if given input is a matrix.\n",
    "  1. In list[0][1] =>In numpy[0,1] -> means In first box 2nd ele.\n",
    "*pd.df.to_numpy() => converts pandas dataframe into numpy array -> ndarray - usally 2 dim ->\n",
    "  1. dataframe -> some kind of tabular format with some indexes. ; numpy array -> some kind of lists of lists\n",
    "  2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GL78P7u0I1Hp"
   },
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28T5zBUCUn4j"
   },
   "source": [
    "## Scaling the pixels between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2MJ1pnPJCyP"
   },
   "outputs": [],
   "source": [
    "x = faces.astype('float32')\n",
    "x = x / 255.0 #Dividing the pixels by 255 for normalization  => range(0,1)\n",
    "\n",
    "# Scaling the pixels value in range(-1,1)\n",
    "x = x - 0.5\n",
    "x = x * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "foWH2FX3JErZ",
    "outputId": "6beda3b8-2474-47b7-e1ab-e9ffbe109903"
   },
   "outputs": [],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b_QhWHs_JGMx",
    "outputId": "7af78ebf-4fbb-432d-ddeb-f784d0eea509"
   },
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "8F5u4B83NKFy",
    "outputId": "8804c429-f539-486f-e01d-0afa62f7a319"
   },
   "outputs": [],
   "source": [
    "plt.plot(x[0,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Euq0pES3Ufv_",
    "outputId": "3c2a1ccc-051e-4ce0-fb47-2c4fbad5543f"
   },
   "outputs": [],
   "source": [
    "print(x.min(),x.max()) # we can observe that pixels are scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZARK_oB_IuF"
   },
   "source": [
    "## Splitting the dataset into train & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oIyb2zMU2sj"
   },
   "outputs": [],
   "source": [
    "num_samples, num_classes = emotions.shape\n",
    "\n",
    "num_samples = len(x)\n",
    "num_train_samples = int((1 - 0.2)*num_samples)\n",
    "\n",
    "# Traning data\n",
    "train_x = x[:num_train_samples]\n",
    "train_y = emotions[:num_train_samples]\n",
    "\n",
    "# Validation data\n",
    "val_x = x[num_train_samples:]\n",
    "val_y = emotions[num_train_samples:]\n",
    "\n",
    "train_data = (train_x, train_y)\n",
    "val_data = (val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oBtesONJWvDY",
    "outputId": "59b1c001-692f-4e86-f641-a01be111402f"
   },
   "outputs": [],
   "source": [
    "print('Training Pixels',train_x.shape)  # ==> 4 dims -  no of images , width , height , color\n",
    "print('Training labels',train_y.shape)\n",
    "\n",
    "print('Validation Pixels',val_x.shape)\n",
    "print('Validation labels',val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3u-V2UMmKTI"
   },
   "source": [
    "* shape => returns no of samples , classes ; len => no of samples\n",
    "* training -> x = x[: 0.8*no_of_samples] , y = y[: 0.8*no_of_samples]     ---> take the benefit of slicing\n",
    "* validation -> x = x[ 0.8*no_of_samples  : ] , y = y[ 0.8*no_of_samples  : ]\n",
    "* data = (x,y)\n",
    "\n",
    "* take benefit of type(), print() --> for checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyZvjJKTAMpx"
   },
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKpD2mbkD3V0"
   },
   "outputs": [],
   "source": [
    "#load the libaray to built the model\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f_8pLj1p85S"
   },
   "source": [
    "* keras is a deep learning library => powerful easy to use library => high level API for tensorflow, Theano => wrapper\n",
    "* minimalistic , modular approach\n",
    "* obviously,oversimplification ->abstract representations of neural networks\n",
    "* import \n",
    "  1. Sequential => linear stack of NN layers -> feed forward cnn\n",
    "  2. layers -> for almost any nn\n",
    "  3. CNN layers -> convolution2D , maxpooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Jg7wHibYv2wF",
    "outputId": "628f6660-4454-4595-92c6-8dc51129aebc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* keras.__version__\n",
    "* pip install --upgrade keras\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dyPOvVbXtSUi"
   },
   "source": [
    "* Deep learning led to advances in computer vision\n",
    "* Neural networks learn more complex features from input image.\n",
    "  1. input layer => feeding input\n",
    "  2. first hidden layer =>  only learn local edge patterns\n",
    "  3. each subsequent layer => filters => learns more complex representations\n",
    "  4. classify\n",
    "\n",
    "* CNN => reduce no of parameters that need to be tuned => handle high dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lni4RvfCDtgD"
   },
   "source": [
    "# 1) Simpler CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-h71E2cTFWOi"
   },
   "outputs": [],
   "source": [
    "input_shape=(48, 48, 1)\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgnHh0urDr7U"
   },
   "outputs": [],
   "source": [
    "\"\"\" Building up Model Architecture \"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same',\n",
    "                            name='image_array', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "\n",
    "\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Activation('softmax',name='predictions'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOB8szvd_tfd"
   },
   "source": [
    " # **Intuition**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "* **Convolution** => matrix multiplication with filter -> feature detector\n",
    "  * Convolution2d => for filtering windows of 2 dimensional input -> if 1st layer = input_shape\n",
    "* filters = windows ; 2 dim filter => sliding window\n",
    "* sliding window slides over each channel and summarising the features.\n",
    "*  **batch normalisation** => used to stabilise perhaps accelerate learning process => standardise layer inputs => by applying transformation that maintains mean activation close to 0 and activation standard deviation(sq root of variance ---> how far) close to 1.\n",
    "  1. normalisation => process tend to follow bell shape curve known as normal distribution\n",
    "  2. backpropagation => updated layer by layer backward from output to the input using estimate of error that assumes weights in the layer prior to the current layer are fixed.\n",
    "  3. gradient tells how to update each parameter under the assumption that other layers do not change. \n",
    "  4. all layers changes during an update --> this update procedure leads to forever chasing a moving target.\n",
    "  5. batch normalisation => technique to coordinate the update  of multiple layers in the model => reparametrization of network\n",
    "  6. its about standardize the mean and variance of each unit in normal dist\n",
    "  7. its all about standardixeing inputs to layers for each mini batch\n",
    "\n",
    "* **Activation(ReLu)**: activation layer( non linear layer) \n",
    "  1. convention is to apply after conv layer\n",
    "  2. to introduce non linearity to a system that has computed linear operations in conv\n",
    "  3. Rectified linear unit => widely used  than non linear functions(sigmoid, tanh) for its fast training with out accuracy. => max(0,x)\n",
    "  4. Relu also alleviates vanishing gradient (lower layers of network trains very slowly because the gradient decreases slowly throught layers.\n",
    "  5. without these non linear functions(activation functions) , the network would be a large linear classifier that could simplified by multiplying weight matrices(accounting for bais) . It wouldn't do anything interesting such as image classification etc..\n",
    "* **Pooling** => max, avg ,globalmax, globalavg\n",
    "  1. conv > activation > pooling\n",
    "  2.to reduce dimensions of feature map => reduces parameters to learn and amt of computations\n",
    "  3. it further summarizes the feature map  instead of precisely positioned features generated by conv layer. This makes model more robust to variations in the position of features in image\n",
    "  4.when network wants to detect higher level features from low level building blocks (detecting corners from edges) . we dont need a rigid about exact position => we need translational invariance at the feature level . so insert pooling\n",
    "  5. overcomes the problem of sensitive to the location of the features.\n",
    "  6. local translation invariance\n",
    "\n",
    "*  **Dropout** => regularization technique\n",
    "  1.neurons are randomly dropped while training\n",
    "  2. this effect makes network less sensitive to thespecific weights of neurons \n",
    "  3. better generalization - less overfit\n",
    "\n",
    "\n",
    "---\n",
    "**Functions:**\n",
    "* convolution2d => no of filters or kernels , kernal size , padding => same(zero padding) , valid (no padding) for input ; stride = step or movement of kernel. ; if 1st layer -> input_shape\n",
    "* BatchNormalisation => no args \n",
    "* Activation => func name\n",
    "* AveragePooling2D => 2D means 2 dimensional feature map ; kernel size , padding\n",
    "* Dropout => % of drop of neurons \n",
    "* AT output , convolution2d --> filters = no of classes ; activation --> softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "l1DSW_zoN1hg",
    "outputId": "5bf2c2fd-78fe-44c1-8822-66a5bb757216"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qS2a73LMMXsF"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 32 #Number of samples per gradient update\n",
    "num_epochs = 200 # Number of epochs to train the model.\n",
    "#input_shape = (64, 64, 1)\n",
    "verbose = 1 #per epohs  progress bar\n",
    "num_classes = 7 \n",
    "patience = 50\n",
    "base_path = 'drive/Colab Notebooks/emotion/simplecnn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZGzV6XKVkmr"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIB613fzVrlK"
   },
   "source": [
    "## Data Augmenttion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0vSiW0aVWoi"
   },
   "outputs": [],
   "source": [
    "\"\"\" Data Augmentation => taking the batch and apply some series of random transformations (random rotation, resizing, shearing) \n",
    "\n",
    "      ===> to increase generalizability of model  \"\"\"\n",
    "\n",
    "\n",
    "# data generator Generate batches of tensor image data with real-time data augmentation\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIW7y7yWkwJ3"
   },
   "source": [
    "**ImageDataGenerator**\n",
    "* horizontal and vertical shift => moving all pixels of image in one direction \n",
    "  1. width_shift_range ( horizonatal shift)\n",
    "  2. height_shift_range (vertical shift)\n",
    "  3. floating num [0- 1] --> % of shift\n",
    "\n",
    "* horizontal n vertical flips augmentation ==>reversing rows or cols of pixels  --> True or False\n",
    "* Random rotation  --> 0 - 360 degrees --> rotation_range = 90 ==> means random rotation to image blw 0 and 90 degrees\n",
    "* random  brightness --> randomly darkens or brightens images ==> brightness_range =[0.2,1.0] --> means darkens or brightens if pixel is blw 0.2 and 1\n",
    "* random zoom\n",
    "  1. either adds pixel or subtract pixels in image . [1-value, 1+value] \n",
    "  2. for example , zoom_range = .3 --> range [0.7, 1.3] or blw 70%(zoom in) and 130% (zoomout)\n",
    "\n",
    "---\n",
    "when an object is created using following args. an iterator can be created for an image dataset. \n",
    "* it iterates through all images in memory --> obj.flow(X,y)\n",
    "* to iterates images through subdirectories --> obj. flow_from_directory(X,y,..)\n",
    "* to train ==> fit_generator() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oeF9vG6oWMB6",
    "outputId": "b68c3b98-06c1-46d3-951c-af8531bd75c1"
   },
   "outputs": [],
   "source": [
    "# model parameters/compilation\n",
    "\n",
    "\"\"\" CONFIGURATION ==>.compile(optimizer, loss , metrics) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EZP8riBlO4mS",
    "outputId": "8ab48817-fcb3-4507-df52-97b59527b165"
   },
   "outputs": [],
   "source": [
    "datasets = ['fer2013']\n",
    "num_epochs = 30\n",
    "base_path=\"/content\"\n",
    "for dataset_name in datasets:\n",
    "    print('Training dataset:', dataset_name)\n",
    "\n",
    "    #callbacks\n",
    "    log_file_path = dataset_name + '_emotion_training.log'\n",
    "\n",
    "    csv_logger = CSVLogger(log_file_path, append=False)\n",
    "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,patience=int(patience/4), verbose=1)\n",
    "    \n",
    "    trained_models_path = base_path + dataset_name + 'simple_cnn'\n",
    "    model_names = trained_models_path + '.{epoch:02d}-{val_loss:.2f}.hdf5'      # if error \"acc\" in 1 line ... don't confuse check entire block since fit() generates a inner loop\n",
    "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "    my_callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "    # loading dataset\n",
    "    train_faces, train_emotions = train_data\n",
    "    history=model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
    "                                            batch_size),\n",
    "                        epochs=num_epochs, verbose=1\n",
    "                        ,callbacks=my_callbacks,validation_data =val_data)   #not callbacks = [my_callbacks] since we my_callbacks is already a list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py6W0E8z6SId"
   },
   "source": [
    "**keras**\n",
    "* **callbacks** => an object that can perform actions at various stages of training\n",
    "      1. write tensorflowboard logs after every batch\n",
    "      2. periodically save model to disk\n",
    "      3. do early stopping \n",
    "      4. view on internal states and statistics during training\n",
    "      * used in fit() loop\n",
    "  * **CSVLogger(filename, separator=\",')**  --> to save epoch results to a csv file\n",
    "      * create obj and use that obj in fit(callbacks=[csv_logger_obj])\n",
    "  * **EarlyStopping()** --> stop training when a monitored metric \n",
    "  has stopped improving\n",
    "      1. monitor = \"val_loss\" --> loss function to be monitored\n",
    "      2. min_delta --> minimum change to count(threshold)\n",
    "      3. patience --> no of epochs with no improvement to stop training\n",
    "  * **ReduceLROnPlateau()**--> reduce learning rate when metric has stopped improving\n",
    "      1. monitor, patience, min_delta\n",
    "      2. factor = 0.1 ==> learning rate reduced to 10% (lr*0.1)\n",
    "      3. verbose ==> 0: quiet , 1: update msgs\n",
    "  * **ModelCheckpoint()** -->to save  keras model or model weights at some frequency\n",
    "      1. filepath\n",
    "      2. monitor --> val_acc or val_loss\n",
    "      3. save_best_only = True \n",
    "\n",
    "---\n",
    "fit_generator(.flow(X,y, batchsize), verbose,epochs,validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aTQpoDjDdsU"
   },
   "source": [
    "* fit() ==> training loop\n",
    "* logs ==> dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vI4nPHW1WZ4r",
    "outputId": "13b47143-8b6f-439f-c47c-49586042f5e3"
   },
   "outputs": [],
   "source": [
    "#evaluate() returns [loss,acc]\n",
    "score = model.evaluate(val_x, val_y, verbose=1) \n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXedkSpQncmu"
   },
   "source": [
    "**history** ====> default callbacks that is registered when training \n",
    "\n",
    "        1.  records training metrics for each epoch\n",
    "        2. these metrics stored in dictionary in history member of object returned\n",
    "        3. obj returns from calls to fit() used to train model \n",
    "        4. data collected in history obj  used to create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MzP_WR5DqZJE",
    "outputId": "2c2e0a43-25ce-4ab6-b895-a71d3020db9e"
   },
   "outputs": [],
   "source": [
    "\"\"\" metrics collected by history object \"\"\"\n",
    "history_dict=history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rxGe4TkroeLK",
    "outputId": "1c1d92a6-f043-4b45-bcbe-187d517110ec"
   },
   "outputs": [],
   "source": [
    "print(history_dict[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "gzqA8Zq-UyqL",
    "outputId": "93c787ed-30af-49b2-ca15-9a8f36937b67"
   },
   "outputs": [],
   "source": [
    "\"\"\" Visualising model training history \"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEe3qvhv5ZA9"
   },
   "outputs": [],
   "source": [
    "train_acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qN9xk7ThKwQ"
   },
   "source": [
    "## Testing the model on  some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2k1XVnmqbCNu"
   },
   "outputs": [],
   "source": [
    " emotion_dict = {0: \"Neutral\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprised\", 6: \"Neutral\"}\n",
    "\n",
    "  #emojis unicodes #\n",
    " emojis = { 0:\"\\U0001f620\",1:\"\\U0001f922\" ,2:\"\\U0001f628\" ,3:\"\\U0001f60A\" , 4:\"\\U0001f625\" ,5:\"\\U0001f632\",6:\"\\U0001f610\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q9Tm7hlE7953",
    "outputId": "f29127d1-c3c3-48e3-86b1-f7ba972cec0c"
   },
   "outputs": [],
   "source": [
    "print(emojis.values(),sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "WC8haBpdsQnR",
    "outputId": "40a2798d-9e5b-4379-db50-095e56946203"
   },
   "outputs": [],
   "source": [
    "!cd content\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNlsHugdbC4C"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "\n",
    "def _predict(path):\n",
    "  facecasc = cv2.CascadeClassifier('/content/haarcascade_frontalface_default.xml')\n",
    "  imagePath = '/content/'+path\n",
    "  image = cv2.imread(imagePath)\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=10)\n",
    "  print(\"No of faces : \",len(faces))\n",
    "  i = 1\n",
    "  for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    roi_gray = gray[y:y + h, x:x + w]                      #croping\n",
    "    cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "    prediction = model.predict(cropped_img)\n",
    "\n",
    "    maxindex = int(np.argmax(prediction))\n",
    "    print(\"person \",i,\" : \",emotion_dict[maxindex], \"-->\",emojis[maxindex])\n",
    "    cv2.putText(image, emotion_dict[maxindex], (x+10, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)  \n",
    "                                              #if text is not apeared , change coordinates. it may work\n",
    "  \n",
    "  cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OXxZnpF1BliC",
    "outputId": "c9629700-4fe2-48d2-f39a-f9e475d66c88"
   },
   "outputs": [],
   "source": [
    "_predict(\"images\\me.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "colab_type": "code",
    "id": "zyxz0H92l7WR",
    "outputId": "fb835241-d7df-445b-df27-4d2821b68e24"
   },
   "outputs": [],
   "source": [
    "_predict(\"images\\ntr.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "zuVB1zlnpspz",
    "outputId": "0e3e3fae-2783-448d-9fb1-34120bad401a"
   },
   "outputs": [],
   "source": [
    "_predict(\"images\\mb.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "Wxz-mznjqEYj",
    "outputId": "80f92cd4-5d62-4f47-e269-15c3fd7e0503"
   },
   "outputs": [],
   "source": [
    "_predict(\"images\\Ram_Charan.webp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "id": "9D-htgZ4v1df",
    "outputId": "8eb5de24-67ec-4910-b645-79ffd673163d"
   },
   "outputs": [],
   "source": [
    "_predict(\"images\\khans.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "S6BEadDySv3v",
    "outputId": "09b33913-cb35-4c76-b65c-f2df082acf5e"
   },
   "outputs": [],
   "source": [
    "_predict(\"images\\me3.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpQDSjHYsa6Q"
   },
   "source": [
    "**Intuition or insights** \n",
    "---\n",
    "* cv2.imshow()\n",
    " is incampatible with google colabs ..we use cv2_imshow() from google.colab.patches \n",
    "* Ipython . Image can also be used to display images in google colabs\n",
    "* we can preprocess the testing image using opencv\n",
    "  * opencv() has so many functionalities\n",
    "    1. imread() --> img path\n",
    "    2. cvtColor() --> img , cv2.COLOR_BGR2GRAY\n",
    "    3. cascadeClassifier() --> detectMultiScale() --> to detect multiple faces in image\n",
    "    4. rectangle() --> img, startpoint, endpoint, color,thickness\n",
    "    5. putText() -->img, text, coordinates, font, fontScale, color, thickness, lineType\n",
    "\n",
    "---\n",
    "* np.argmax(predictions) ==> to get the maximum confidence prediction\n",
    "* croping of image --> use the power of slicing\n",
    "* cascade classifier ==> ensemble learning based on concatenation of several classifiers\n",
    "    * not multiexpert --> multistage\n",
    "    * combinatorial nature of the classification\n",
    "* haar cascade classifier ==> highly pretrained models stored in xml files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nuLnxqYq-E6"
   },
   "source": [
    "**Error log :**\n",
    "\n",
    "1. opencv .. !_src.empty() in function \"cvtColor\" ===> it happends if there is a wrong in previous stmt --> .imread()\n",
    "2. opencv .. !empty() in function detectMultiScale ===> xml file is missing\n",
    "3. cannot create group in read only mode ===> model is not defined but loaded \n",
    "4. model is not defined ===> define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fZ16t7K8iRCS",
    "outputId": "2a13490b-8c50-4ed9-a435-5928c04d2eeb"
   },
   "outputs": [],
   "source": [
    "\"\"\" code snippet to display image \"\"\"\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('/content/me.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNB2QgwJLA-j"
   },
   "source": [
    "* **saving the weights of the model and again loading it in keras**\n",
    "* **saving the architecture of the model in json file using model_from_json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pgRxjAyjzq0"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0lD0bddwj1Sx"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N600QexbpCJd",
    "outputId": "8f64c3aa-80ba-4e1b-c65b-59875d76b6b6"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6mmEsNVpCMv"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "#model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJgyEXKRogSZ"
   },
   "outputs": [],
   "source": [
    "\"\"\" code snippet for Downloading files from colabs \"\"\"\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"model.h5\")\n",
    "files.download(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8D_G0SuqOeA1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJxyoe0SpC2k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s38ekv_fqSwP"
   },
   "outputs": [],
   "source": [
    "\"\"\" loading the model in modular approach \"\"\"\n",
    "\n",
    "def load_model_():\n",
    "  json_file = open('model.json', 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  model = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  model.load_weights(\"model.h5\")\n",
    "  return model\n",
    "\n",
    "\n",
    "model = load_model_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM4bcS9EqKNt"
   },
   "outputs": [],
   "source": [
    "model2 = load_model_()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Facial Emotion Recognition using CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
